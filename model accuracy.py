# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1l9AYU0NUhb3LolH2EqM0FSSDdbkCJ1My
"""

import pandas as pd # for data manipulation
import numpy as np # for numerical operations
import os # for file path operations

import matplotlib.pyplot as plt # for plotting
import seaborn as sns # for plotting

import json # for saving results
from datetime import datetime # for date operations

# machine learning libraries from sklearn for Logistic Regression, Decision Tree, Random Forest, and Neural Network
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier # for neural network


#------------------------------------------------------------------------------------------------------------


#Data Loading & Preprocessing

print("Data Loading\n")

# For storing results
if not os.path.exists('results'):
    os.makedirs('results')
    print("New folder created: 'results'\n")


# Loading CSV file
matches = pd.read_csv('data/ChampionsTrophy_Historical.csv')
print("CSV file loaded\n")

# Converting to date format
matches['Match Date'] = pd.to_datetime(matches['Match Date'])
print("Date format converted\n\n")


print(matches.head(), "\n")  # Just to see a few rows
print("our data preview\n")

# Storting data by date
matches_sorted = matches.sort_values('Match Date')

#checking champion for each season based on the last match in that season ( data validation )
actual_champions = matches_sorted.groupby('Season').last().reset_index() # searching the last match
actual_champions_dict = dict(zip(actual_champions['Season'], actual_champions['Winner']))
print("\nOur dictionary of {season: champion}:\n")
print(actual_champions_dict, "\n")


#------------------------------------------------------------------------------------------------------------


#Team-Level Metrics (Matches, Wins, etc.)

print("\nDataFrame of team-level statistics...")

# All seasons and all teams from the data
seasons = sorted(matches['Season'].unique())
teams = pd.unique(matches[['Team1', 'Team2']].values.ravel('K'))
# All unique teams

# count their matches played, wins, losses, etc for each team in each season
print("Calculating matches played, wins, losses, and win percentage for each team in each season.")

records = []

for season in seasons:
    # Only for known champion
    if season not in actual_champions_dict:
        continue

    season_matches = matches[matches['Season'] == season]

    # Going through all teams, to check if they participated this season
    for team in teams:
        played = ((season_matches['Team1'] == team) |
                  (season_matches['Team2'] == team)).sum()
        wins   = (season_matches['Winner'] == team).sum()
        losses = played - wins  # ignoring draws/no-results

        # Win percentage is (wins / played) * 100 or 0 if no matches played
        win_pct = (wins / played) * 100 if played > 0 else 0

        records.append({
            'Season': season,
            'Team': team,
            'Matches': played,
            'Wins': wins,
            'Losses': losses,
            'Win_Percentage': win_pct
        })

#Converting list of dictionaries into a Pandas DataFrame
team_metrics = pd.DataFrame(records)

# Removing rows for teams who did not participate (Matches == 0)
team_metrics = team_metrics[team_metrics['Matches'] > 0]

#Adding a "Champion" column that flags whether a team won in that season (1) or not (0)
def is_champion(row):
    s = row['Season']
    t = row['Team']
    return 1 if (s in actual_champions_dict and actual_champions_dict[s] == t) else 0

team_metrics['Champion'] = team_metrics.apply(is_champion, axis=1)

print("\n\nHere are some example rows from our new team-level DataFrame:")
print(team_metrics.head(), "\n")


#------------------------------------------------------------------------------------------------------------


# Exploratory Data Analysis
print("\n\nPlotting some graphs to understand our data...\n")

# For distribution of Win_Percentage
print("Plotting the distribution of team win percentages...")
plt.figure(figsize=(10,5))
plt.hist(team_metrics['Win_Percentage'], bins=20, color='skyblue', edgecolor='black')

plt.xlabel("Win Percentage (%)")
plt.ylabel("Number of Team-Seasons")
plt.title("Distribution of Team Win Percentages across Seasons")
plt.tight_layout()

plt.savefig('results/win_percentage_distribution.png')
plt.show()

# For average Wins per Season
print("\n\nPlotting the average number of wins (per team) for each season...\n")
avg_wins = team_metrics.groupby('Season')['Wins'].mean()

plt.figure(figsize=(10,5))
avg_wins.plot(marker='o', color='green')

plt.xlabel("Season")
plt.ylabel("Average Wins (per Team)")
plt.title("Average Wins per Season")
plt.grid(True)
plt.tight_layout()
plt.savefig('results/avg_wins_per_season.png')
plt.show()



# For countplot of Team's Participation
print("\n\nPlotting how many seasons each team has participated in (countplot)...\n")
plt.figure(figsize=(12,6))
sns.countplot(data=team_metrics, x='Team', order=team_metrics['Team'].value_counts().index)
plt.xticks(rotation=45)
plt.title("Number of Seasons Each Team Has Participated In")
plt.xlabel("Team")
plt.ylabel("Count of Seasons Participated")
plt.tight_layout()
plt.savefig('results/team_participation.png')
plt.show()


#------------------------------------------------------------------------------------------------------------


# Model Training & Prediction (Time-based)
print("\n\nTraining several models and predicting champions season by season...")

# Defining which columns are our 'features' (inputs).
features = ['Matches', 'Wins', 'Losses', 'Win_Percentage']

# Defining which is the target (Champion or not).
target = 'Champion'



####################
# Creating instances of the four machine learning  models that we are comparing
models = {
    'LogisticRegression': LogisticRegression(max_iter=1000),
    'DecisionTree': DecisionTreeClassifier(random_state=42),
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'NeuralNetwork': MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
}
####################

print("\n\nModels to compare:", list(models.keys()))

# Storing detailed results for each season in a list
results_list = []



# ne
# Tracking each model's perforormance on the training set vs. the test set to check if it's overfitting.
train_accuracies = {m: [] for m in models}
test_accuracies  = {m: [] for m in models}
seasons_used_for_training = []

# Only considering the seasons that have a known champion (to avoid incomplete data)
valid_seasons = sorted([s for s in seasons if s in actual_champions_dict])



# For each season : training on all previous seasons and testing on the current season.
for i, season in enumerate(valid_seasons):
    #  Skipping it there's no prior data to train on ( very first season in the list )
    if i == 0:
        continue

    # Training seasons are all valid seasons that come before the current season
    train_seasons = valid_seasons[:i]
    test_season   = season

    # Collecting the rows from team_metrics that correspond to training seasons
    train_data = team_metrics[team_metrics['Season'].isin(train_seasons)]

    # Collecting the rows that correspond to our current test season
    test_data  = team_metrics[team_metrics['Season'] == test_season].copy()

    # Storing the actual champion for the test season
    season_result = {
        'Season': test_season,
        'ActualChampion': actual_champions_dict[test_season]
    }
    seasons_used_for_training.append(test_season)

    print(f"\nNow processing Season {test_season} (training on: {train_seasons})...")

    # For each model, training and evaluating
    for model_name, model in models.items():
        print(f"  Training {model_name}...")

        # Training the model on the training data
        X_train = train_data[features]
        y_train = train_data[target]
        model.fit(X_train, y_train)


  #------------------------------------------------------------------------------------------------------------


    # Calculating the Training Accuracy

        correct_count_train = 0
        for s in train_seasons:
            s_df = train_data[train_data['Season'] == s].copy()

            if hasattr(model, "predict_proba"):
                s_df['Pred_Prob'] = model.predict_proba(s_df[features])[:, 1]
            else:
                s_df['Pred_Prob'] = model.predict(s_df[features])

            # The team with the max predicted probability is considered the predicted champion
            best_idx = s_df['Pred_Prob'].idxmax()
            predicted_champion = s_df.loc[best_idx, 'Team']

            if predicted_champion == actual_champions_dict[s]:
                correct_count_train += 1

        # This is the fraction of training seasons where the model picked the right champion
        train_accuracy = (correct_count_train / len(train_seasons)
                          if len(train_seasons) > 0 else 0)
        train_accuracies[model_name].append(train_accuracy)


#------------------------------------------------------------------------------------------------------------


   # Calculating the Test Accuracy

        # Checking if the model can pick the champion from the test season
        if hasattr(model, "predict_proba"):
            test_data['Pred_Prob'] = model.predict_proba(test_data[features])[:, 1]
        else:
            test_data['Pred_Prob'] = model.predict(test_data[features])

        best_idx = test_data['Pred_Prob'].idxmax()
        predicted_champion = test_data.loc[best_idx, 'Team']
        actual_champion = actual_champions_dict[test_season]

        # 1 if correct, 0 if incorrect
        test_correct = 1 if predicted_champion == actual_champion else 0

        # Storing the result in our dictionary
        season_result[model_name] = {
            'PredictedChampion': predicted_champion,
            'Probability': test_data.loc[best_idx, 'Pred_Prob'],
            'Correct': test_correct
        }

        # Tracking this test correctness so we can average later
        test_accuracies[model_name].append(test_correct)

        print(f"     {model_name} predicted champion: {predicted_champion}, "
              f"actual champion: {actual_champion} (Correct? {bool(test_correct)})")

    # Including this season's result (with all models) to our results list
    results_list.append(season_result)

print("\nDone looping over all seasons for training and testing!\n")


#------------------------------------------------------------------------------------------------------------


# Model Performance Summary
print("STEP 5: Summarizing overall test accuracy for each model...")

# Computing the average of the 1|0 correctness values in test_accuracies for each model
accuracy_summary = {}
for model_name in models:
    # test_accuracies[model_name] is a list of 1/0 for each test season
    overall_acc = np.mean(test_accuracies[model_name]) if len(test_accuracies[model_name]) > 0 else 0
    accuracy_summary[model_name] = round(overall_acc * 100, 2)

print("\nOverall Test Accuracy (Champion Prediction) by Model (%):")
print(accuracy_summary)

# Printing them more clearly
for m_name, acc in accuracy_summary.items():
    print(f" * {m_name}: {acc}%")


#------------------------------------------------------------------------------------------------------------

# comment for Subrat - bro this will get us all the graphs we need that I can think off,
# can you check for the graphgs and tell me what else you need
#  by - vineet



# Visualization of Predictions
print("\n\n Creating and saving final plots...\n")




# Actual plot vs. Predicted Champions per Season
print("\n\n - Creating a line chart of actual vs. predicted champions by season...")
season_list            = [res['Season'] for res in results_list]
actual_champions_list  = [res['ActualChampion'] for res in results_list]

plt.figure(figsize=(12, 6))
plt.plot(season_list, actual_champions_list, marker='o',
         label='Actual Champion', linestyle='--')

for model_name in models:
    pred_champs = [res[model_name]['PredictedChampion'] for res in results_list]
    plt.plot(season_list, pred_champs, marker='o',
             label=model_name, linestyle='-')

plt.xlabel("Season")
plt.ylabel("Champion Team (Categorical)")
plt.title("Actual vs. Predicted Champions per Season")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('results/champion_predictions.png')
plt.show()

#everything looks good, baki sab theek hi hai
# Vineet can you add a chart for model accurecy or something? -
#thankyou - Subrat


# vineet - chart uodate resolved

# 2 Bar Chart of Overall Test Accuracy by Model
print(" - Creating a bar chart of overall test accuracies for each model...")
model_acc_pairs = sorted(accuracy_summary.items(), key=lambda x: x[1], reverse=True)
models_order, accuracy_vals = zip(*model_acc_pairs)

plt.figure(figsize=(8,5))
sns.barplot(x=list(models_order), y=list(accuracy_vals), palette='viridis')
plt.ylim(0, 100)
plt.xlabel("Model")
plt.ylabel("Accuracy (%)")
plt.title("Overall Test Accuracy by Model (Champion Prediction)")
plt.tight_layout()
plt.savefig('results/model_accuracies_bar.png')
plt.show()



# 3 Training vs. Test Accuracy by Season
print(" - Creating a line chart of train vs. test accuracy per season to check for overfitting...")
df_train_acc = pd.DataFrame(train_accuracies, index=seasons_used_for_training)
df_test_acc  = pd.DataFrame(test_accuracies, index=seasons_used_for_training)

plt.figure(figsize=(10,5))
for model_name in models:
    # Plotting training accuracy with dashed line
    plt.plot(df_train_acc.index, df_train_acc[model_name],
             marker='o', linestyle='--', label=f'{model_name} - Train')

    # Plotting test accuracy with solid line
    plt.plot(df_test_acc.index, df_test_acc[model_name],
             marker='o', linestyle='-',  label=f'{model_name} - Test')


plt.title("Train vs. Test Accuracy by Season (Champion Prediction)")
plt.xlabel("Season")
plt.ylabel("Accuracy (Correct Champion = 1, Otherwise = 0)")
plt.legend()
plt.xticks(rotation=45)
plt.ylim(-0.05, 1.05)
plt.grid(True)
plt.tight_layout()
plt.savefig('results/train_test_accuracy.png')
plt.show()


print("\nAll plots have been saved in the 'results' folder!")

print("DONE. You have successfully trained multiple models to predict the champion each season.")
print("====== Program Finished EXIT ========")



# Vineet Singh, Subrat Acharya , Ratna Kirti

#------------------------------------------------------------------------------------------------------------