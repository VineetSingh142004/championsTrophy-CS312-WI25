# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AjREkAX1wegXjLcB7lDVjQva6LVKVuOr
"""

# -*- coding: utf-8 -*-
"""champion_prediction_future_year.py

This code trains multiple models (Logistic Regression, Decision Tree,
Random Forest, Neural Network) to predict the champion of each season
based on historical data. Then it lets the user specify a future year
(after 2025, in increments of 2) and provides a naÃ¯ve prediction
for that future year's champion.
"""

import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import seaborn as sns
import json
from datetime import datetime

# Sklearn ML models
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier

#------------------------------------------------------------------------------------------------------------
# 1. Data Loading & Preprocessing
print("Data Loading\n")

# Create 'results' folder if it doesn't exist
if not os.path.exists('results'):
    os.makedirs('results')
    print("New folder created: 'results'\n")

# Load your historical CSV file
matches = pd.read_csv('data/ChampionsTrophy_Historical.csv')
print("CSV file loaded\n")

# Convert to datetime format
matches['Match Date'] = pd.to_datetime(matches['Match Date'])
print("Date format converted\n")

print(matches.head(), "\n")  # Preview first few rows

# Sort data by date
matches_sorted = matches.sort_values('Match Date')

# Identify actual champions by looking at the last match in each season
actual_champions = matches_sorted.groupby('Season').last().reset_index()
actual_champions_dict = dict(zip(actual_champions['Season'],
                                 actual_champions['Winner']))
print("\nDictionary of {season: champion}:\n", actual_champions_dict, "\n")

#------------------------------------------------------------------------------------------------------------
# 2. Team-Level Metrics (Matches, Wins, etc.)

print("Creating a DataFrame of team-level statistics per season...")

seasons = sorted(matches['Season'].unique())
teams = pd.unique(matches[['Team1', 'Team2']].values.ravel('K'))  # All unique teams

records = []
for season in seasons:
    # Only consider seasons with known champion
    if season not in actual_champions_dict:
        continue

    season_matches = matches[matches['Season'] == season]

    for team in teams:
        played = ((season_matches['Team1'] == team) |
                  (season_matches['Team2'] == team)).sum()
        wins   = (season_matches['Winner'] == team).sum()
        losses = played - wins  # ignoring draws/no-result
        win_pct = (wins / played) * 100 if played > 0 else 0

        records.append({
            'Season': season,
            'Team': team,
            'Matches': played,
            'Wins': wins,
            'Losses': losses,
            'Win_Percentage': win_pct
        })

team_metrics = pd.DataFrame(records)
team_metrics = team_metrics[team_metrics['Matches'] > 0]  # drop teams w/ 0 matches

def is_champion(row):
    s = row['Season']
    t = row['Team']
    return 1 if (s in actual_champions_dict and actual_champions_dict[s] == t) else 0

team_metrics['Champion'] = team_metrics.apply(is_champion, axis=1)

print("\nSample rows from the team_metrics DataFrame:\n", team_metrics.head(), "\n")

#------------------------------------------------------------------------------------------------------------
# 3. Exploratory Data Analysis (Optional Plots)

print("Plotting some graphs...\n")

# 3.1 Distribution of Win_Percentage
plt.figure(figsize=(10,5))
plt.hist(team_metrics['Win_Percentage'], bins=20, color='skyblue', edgecolor='black')
plt.xlabel("Win Percentage (%)")
plt.ylabel("Number of Team-Seasons")
plt.title("Distribution of Team Win Percentages across Seasons")
plt.tight_layout()
plt.savefig('results/win_percentage_distribution.png')
plt.show()

# 3.2 Average Wins per Season
avg_wins = team_metrics.groupby('Season')['Wins'].mean()
plt.figure(figsize=(10,5))
avg_wins.plot(marker='o', color='green')
plt.xlabel("Season")
plt.ylabel("Average Wins (per Team)")
plt.title("Average Wins per Season")
plt.grid(True)
plt.tight_layout()
plt.savefig('results/avg_wins_per_season.png')
plt.show()

# 3.3 Team participation
plt.figure(figsize=(12,6))
sns.countplot(data=team_metrics,
              x='Team',
              order=team_metrics['Team'].value_counts().index)
plt.xticks(rotation=45)
plt.title("Number of Seasons Each Team Has Participated In")
plt.xlabel("Team")
plt.ylabel("Count of Seasons Participated")
plt.tight_layout()
plt.savefig('results/team_participation.png')
plt.show()

#------------------------------------------------------------------------------------------------------------
# 4. Model Training & Time-based Prediction

print("\nTraining models and predicting champions season by season...\n")

# Features and target
features = ['Matches', 'Wins', 'Losses', 'Win_Percentage']
target = 'Champion'

# Models
models = {
    'LogisticRegression': LogisticRegression(max_iter=1000),
    'DecisionTree': DecisionTreeClassifier(random_state=42),
    'RandomForest': RandomForestClassifier(n_estimators=100, random_state=42),
    'NeuralNetwork': MLPClassifier(hidden_layer_sizes=(50,), max_iter=1000, random_state=42)
}
print("Models to compare:", list(models.keys()))

# Prepare structures to store accuracy results
results_list = []
train_accuracies = {m: [] for m in models}
test_accuracies  = {m: [] for m in models}
seasons_used_for_training = []

# Only consider seasons with known champion
valid_seasons = sorted([s for s in seasons if s in actual_champions_dict])

# Loop through each season, train on all previous (valid) seasons, test on the current
for i, season in enumerate(valid_seasons):
    # Skip first season because there's no prior data to train on
    if i == 0:
        continue

    train_seasons = valid_seasons[:i]
    test_season   = season

    train_data = team_metrics[team_metrics['Season'].isin(train_seasons)]
    test_data  = team_metrics[team_metrics['Season'] == test_season].copy()

    season_result = {
        'Season': test_season,
        'ActualChampion': actual_champions_dict[test_season]
    }
    seasons_used_for_training.append(test_season)

    print(f"\n--- Processing Season {test_season} (training on: {train_seasons}) ---")

    for model_name, model in models.items():
        # Train
        X_train = train_data[features]
        y_train = train_data[target]
        model.fit(X_train, y_train)

        #------------------- TRAINING ACCURACY (by champion-correctness) -----------------#
        correct_count_train = 0
        for s in train_seasons:
            s_df = train_data[train_data['Season'] == s].copy()
            if hasattr(model, "predict_proba"):
                s_df['Pred_Prob'] = model.predict_proba(s_df[features])[:, 1]
            else:
                # For models without predict_proba
                s_df['Pred_Prob'] = model.predict(s_df[features])

            best_idx = s_df['Pred_Prob'].idxmax()
            predicted_champion = s_df.loc[best_idx, 'Team']

            if predicted_champion == actual_champions_dict[s]:
                correct_count_train += 1

        train_accuracy = (correct_count_train / len(train_seasons)
                          if len(train_seasons) > 0 else 0)
        train_accuracies[model_name].append(train_accuracy)

        #--------------------- TEST ACCURACY (by champion-correctness) -------------------#
        if hasattr(model, "predict_proba"):
            test_data['Pred_Prob'] = model.predict_proba(test_data[features])[:, 1]
        else:
            test_data['Pred_Prob'] = model.predict(test_data[features])

        best_idx = test_data['Pred_Prob'].idxmax()
        predicted_champion = test_data.loc[best_idx, 'Team']
        actual_champion = actual_champions_dict[test_season]

        test_correct = 1 if predicted_champion == actual_champion else 0

        season_result[model_name] = {
            'PredictedChampion': predicted_champion,
            'Probability': test_data.loc[best_idx, 'Pred_Prob'],
            'Correct': test_correct
        }

        test_accuracies[model_name].append(test_correct)

        print(f"{model_name} -> predicted champion: {predicted_champion}, actual: {actual_champion} (Correct? {bool(test_correct)})")

    results_list.append(season_result)

print("\nFinished training & testing across all seasons.\n")

#------------------------------------------------------------------------------------------------------------
# 5. Model Performance Summary

accuracy_summary = {}
for model_name in models:
    # test_accuracies[model_name] is a list of 1/0
    overall_acc = np.mean(test_accuracies[model_name]) if len(test_accuracies[model_name]) > 0 else 0
    accuracy_summary[model_name] = round(overall_acc * 100, 2)

print("Overall Test Accuracy (Champion Prediction) by Model:")
for m_name, acc in accuracy_summary.items():
    print(f" * {m_name}: {acc}%")

#------------------------------------------------------------------------------------------------------------
# 6. Visualizations of Model Predictions & Accuracy

print("\nCreating final plots...\n")

# 6.1 Actual vs. Predicted Champions per Season
season_list           = [res['Season'] for res in results_list]
actual_champions_list = [res['ActualChampion'] for res in results_list]

plt.figure(figsize=(12,6))
plt.plot(season_list, actual_champions_list, marker='o', linestyle='--',
         label='Actual Champion')

for model_name in models:
    pred_champs = [res[model_name]['PredictedChampion'] for res in results_list]
    plt.plot(season_list, pred_champs, marker='o', linestyle='-',
             label=model_name)

plt.xlabel("Season")
plt.ylabel("Champion Team")
plt.title("Actual vs. Predicted Champions per Season")
plt.legend()
plt.xticks(rotation=45)
plt.tight_layout()
plt.savefig('results/champion_predictions.png')
plt.show()

# 6.2 Bar chart of overall accuracies
model_acc_pairs = sorted(accuracy_summary.items(),
                         key=lambda x: x[1],
                         reverse=True)
models_order, accuracy_vals = zip(*model_acc_pairs)

plt.figure(figsize=(8,5))
sns.barplot(x=list(models_order), y=list(accuracy_vals), palette='viridis')
plt.ylim(0, 100)
plt.xlabel("Model")
plt.ylabel("Accuracy (%)")
plt.title("Overall Test Accuracy by Model (Champion Prediction)")
plt.tight_layout()
plt.savefig('results/model_accuracies_bar.png')
plt.show()

# 6.3 Training vs. Test Accuracy by Season
df_train_acc = pd.DataFrame(train_accuracies, index=seasons_used_for_training)
df_test_acc  = pd.DataFrame(test_accuracies, index=seasons_used_for_training)

plt.figure(figsize=(10,5))
for model_name in models:
    # Train accuracy (dashed)
    plt.plot(df_train_acc.index, df_train_acc[model_name],
             marker='o', linestyle='--', label=f'{model_name} - Train')
    # Test accuracy (solid)
    plt.plot(df_test_acc.index, df_test_acc[model_name],
             marker='o', linestyle='-',  label=f'{model_name} - Test')

plt.title("Train vs. Test Accuracy by Season (Champion Prediction)")
plt.xlabel("Season")
plt.ylabel("Accuracy (0 or 1)")
plt.legend()
plt.xticks(rotation=45)
plt.ylim(-0.05, 1.05)
plt.grid(True)
plt.tight_layout()
plt.savefig('results/train_test_accuracy.png')
plt.show()

print("\nAll plots saved in 'results' folder.")
print("Models are now trained and evaluated.\n")

#------------------------------------------------------------------------------------------------------------
#Prompt the user for a FUTURE YEAR (after 2025) and predict the champion

next_championship_year = int(input(
    "Enter the next championship year you want to predict after 2025 "
    "(must be at least 2027, in 2-year increments): "
))

# Basic validation: ensure year > 2025 and it's in 2-year steps (2027, 2029, ...)
if next_championship_year <= 2025 or (next_championship_year - 2025) % 2 != 0:
    print(f"\n** Invalid input! Please enter a year like 2027, 2029, 2031, etc. **")
else:
    print(f"\nPredicting champion for the year {next_championship_year}...\n")

    # Use the *last valid season* as a basis for future stats
    last_known_season = valid_seasons[-1]

    # Copy stats from that season for all teams
    future_test_data = team_metrics[team_metrics['Season'] == last_known_season].copy()
    # Label them as if they are playing in the new year
    future_test_data['Season'] = next_championship_year

    # Predict champion for each model
    for model_name, model in models.items():
        if hasattr(model, "predict_proba"):
            future_test_data['Pred_Prob'] = model.predict_proba(future_test_data[features])[:, 1]
        else:
            future_test_data['Pred_Prob'] = model.predict(future_test_data[features])

        # The team with the highest predicted probability is chosen
        best_idx = future_test_data['Pred_Prob'].idxmax()
        predicted_champion = future_test_data.loc[best_idx, 'Team']

        print(f"{model_name} -> Predicted Champion for {next_championship_year}: {predicted_champion}")

print("\nDONE. Program Finished.")